# ğŸš— MountainCar Reinforcement Learning Project

This project applies **Reinforcement Learning (RL)** techniques to solve the classic
**MountainCar-v0** control problem using the **Gymnasium** framework.
Both **Tabular Q-Learning** and **Deep Q-Network (DQN)** algorithms are implemented
and compared.

The MountainCar problem is a benchmark RL task where a car must reach the top of a
steep hill despite limited engine power. The agent must learn to move back and forth
to build momentum before climbing the hill.

---

## ğŸ“Œ Project Objectives

- Understand Reinforcement Learning through interaction with an environment
- Implement and analyze **Tabular Q-Learning**
- Implement **Deep Q-Network (DQN)** for continuous state spaces
- Compare classical RL and Deep RL approaches
- Visualize learned behavior using a custom **Pygame** interface

---

## ğŸ§  Problem Description: MountainCar-v0

### ğŸ”¹ Environment
- **Environment Name:** `MountainCar-v0`
- **Library:** Gymnasium

### ğŸ”¹ State Space
The state consists of two continuous variables:
- Car position
- Car velocity

\[
s = (\text{position}, \text{velocity})
\]

### ğŸ”¹ Action Space
The agent can choose one of the following actions:
- `0` â†’ Push car to the left
- `1` â†’ No push
- `2` â†’ Push car to the right

### ğŸ”¹ Reward Function
- Reward = **âˆ’1** at every time step
- No positive reward is given when the goal is reached

This encourages the agent to minimize the number of steps taken to reach the goal.

### ğŸ”¹ Episode Termination
An episode ends when:
- The car reaches the goal position, or
- The maximum episode length (**200 steps**, default Gymnasium limit) is reached

---

## ğŸ§® Algorithms Implemented

### 1ï¸âƒ£ Tabular Q-Learning

Tabular Q-learning stores expected rewards for each **stateâ€“action pair** in a table.
Because MountainCar has a continuous state space, states are discretized into bins.

#### Q-Learning Update Rule
\[
Q(s, a) \leftarrow Q(s, a) + \alpha
\left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
\]

Where:
- \( \alpha \) = learning rate
- \( \gamma \) = discount factor

#### Limitations
- Requires discretization
- No generalization across states
- Slower convergence in continuous environments

---

### 2ï¸âƒ£ Deep Q-Network (DQN)

DQN replaces the Q-table with a neural network that approximates the Q-function.

#### Neural Network Architecture
- Input layer: 2 neurons (position, velocity)
- Hidden layers: Fully connected with ReLU activation
- Output layer: 3 neurons (Q-values for each action)

#### Stabilization Techniques
- Experience Replay
- Target Network
- Mean Squared Error (MSE) loss

---

## ğŸ“ Project Structure & File Roles

```text
MountainCar/
â”‚
â”œâ”€â”€ README.md
â”‚   â–¶ Project description, explanation, and execution instructions
â”‚
â”œâ”€â”€ main.py
â”‚   â–¶ Environment test script using random actions with rendering
â”‚
â”œâ”€â”€ test_env.py
â”‚   â–¶ Sanity check for environment reset, step, reward, and termination
â”‚
â”œâ”€â”€ q_learning_mountaincar.py
â”‚   â–¶ Tabular Q-learning implementation with state discretization
â”‚
â”œâ”€â”€ q_learning_mountaincar_v2.py
â”‚   â–¶ Improved version of Q-learning
â”‚
â”œâ”€â”€ q_table.npy
â”‚   â–¶ Saved Q-table after training
â”‚
â”œâ”€â”€ dqn_mountaincar.py
â”‚   â–¶ Deep Q-Network (DQN) training script
â”‚
â”œâ”€â”€ dqn_mountaincar.pth
â”‚   â–¶ Saved trained DQN model weights
â”‚
â”œâ”€â”€ dqn_play.py
â”‚   â–¶ Runs the trained DQN agent without exploration
â”‚
â”œâ”€â”€ fancy_mountaincar.py
â”‚   â–¶ Custom Pygame visualization of trained agent
â”‚
â”œâ”€â”€ engine.wav
â”‚   â–¶ Engine sound for visualization
â”‚
â”œâ”€â”€ success.wav
â”‚   â–¶ Sound played when the goal is reached
